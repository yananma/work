
环境：`conda activate mxnlp`  
目录：`/home/test/syb/nlp/mxnlp/run/20.yuche_zh_v9`  

## yuche-builddataset.ipynb  

sents 就是 sentences  

buildsig 是 build signature  

rec 是 record  

#### SentenceSplitter 类  

就是把句子转换成句子放到列表里。每个句子是一个元组，包括句子，和句子的起始结束坐标三个元素。    


#### CanoDoc 类  

lstart 是 label start，lend 是 label end  

delayed_yuche 标志和具体内容不在同一句话里   



## yuche_dataset_check.ipynb  

#### YCDataSet 类  

run_cut() 函数：  

使用 lac 分词，把分词结果赋值给 obj 的 sentence_cut 属性，也就是同时修改了 self.data(修改内容，不修改容器，因为是变量的同一个引用)。  


#### DeDuperb 类  

compute 函数：以 4 个字为窗口，移动窗口取词，计算 hash，函数返回的是 hash 值。  

dedup 函数：传入的 self.data 是列表套列表，内部列表存放的是命中句子的分词结果。  

lincnt 就是多少条数据；data_h 是 hash 值列表；index 是 hash 值为键 lincnt 为值的字典；data_v 是 lincnt 为键分词结果为值的字典；duphash 是重复的 hash 字典，键是 hash 值，值是 lincnt 列表（lincnt 其实应该就是 line id）  

这个 dedup 函数就是去重，一个是 hash 值相同的去重，还有一个 group 去重。  



## run_classifier.py  

#### load_and_cache_examples 函数  

读取数据，  

all_input_ids 每一个向量是一个样本，传入的是 vocab 映射的 id，max_leng=128，不足填充 0.  


#### train 函数  

就是普通的训练函数，前向传播、计算损失、反向求导、更新参数。  




## yuche_extract_builddataset.ipynb  

#### add_bmes 函数  

就是为句子添加 ner 标签。整个 notebook 基本上就是做了这一件事。  



## yuche_extracttrain.ipynb  

#### SentenceGetter 类  

先通过遍历进行分词；把 ner 标签赋值给 label 变量  

sentences 就是分词以后的句子列表；labels 就是 ner 标签。  




最后 accuracy、f1、precision、recall 值的计算是在 clue_classifier/metrics/clue_compute_metrics.py 中。  


## 问题  










