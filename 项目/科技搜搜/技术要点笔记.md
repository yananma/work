
进入虚拟环境  
cd ~  
dofspy37venv  

handle 方法  

先 delete 全部，就是清除以前的内容  

```python
if not obj.strip():
    continue
```
意思就是如果 obj 不存在就跳过这一个  


zj 是专家  

j 是监管  




### 数据  

数据库 zky_posts 是中科院的数据表  

upload_to_community 项目，从 kafka 上传数据到 ES  

upload_to_community 文件夹下的 upload.py，上传数据用的，不是查    

normalize_else 是上传到中科院  


XSite 是所有的有效的域名，取到域名以后，会和这个数据库比对，取到有效的域名  

entry 是当前，site 父域名   





### 项目用到的知识点    

#### connect 包

各种连接方式、ES 还多一个分页方法，这个方法比较重要，用的比较多。  

kafka 有一个生产和消费，send 生产，receive 是消费。get_consumer 指定消费者  

MySQL 就一个连接  

settings 包。  

BaseConnectSetting.py  

`locals().pop('kwargs').items()`，kwargs 就是子类继承的时候传入的参数。    

`dict(list(self.to_iter()))`  

`__import__`  




#### filters 包

全是匹配部分  

base.py 传一组规则，传入规则名，同义词、限定词、反向限定词，主要是查国家用的。高级搜索里使用。key_rules 用于匹配，可以同时匹配多条规则。  

add_key 一组规则，add_keys 多组规则。exec_filter 是查询用的，传入规则  

reverse_keyfilter_result()，把结果转换了一种返回形式。    

esm 传很多关键词，匹配一句话就能把命中的关键词和位置筛出来，在 index 的 AutoIndex 类中。  


#### tools 包

各种工具  

mx_simhash.py 不看  

text_tools.py 扩展的文本处理，有各种各样的处理方法，可以链式调用（返回了 self）（有很大的改进空间）  

threadingadapter.py 多线程，多进程，返回结果队列传个 piplines 中使用    

utils.py 延迟加载，不重要  


#### piplines 包

normalize_data 格式化数据  

process_data 处理数据  


接收数据、处理数据、返回数据，异步运行（多线程，多进程）  

linemodule.py last_step_queue 接收 threadingadapter.py 中处理完成的数据，主要有 3 个方法，read_data 读数据，normalize_data 预处理，process_data 是正规处理数据。  

db_read_module.py 从数据库批量读数据，返回数据  

es_bulk_module.py 批量操作 ES，主要是增删改    

es_craw_module.py 主要是从 ES 中查数据   

forecast_module.py 筛选包好预测关键词的文章，去除不在的  

lac_module.py 提取人名、机构、关键词，专家观点。给产品进行规则分析用的。  

origin_post_module.py 没用，不看  

output_module.py 转换成 Excel 格式。不用看  

zhili_module.py  选取有治理关键词的文章  



#### user 应用

下 utils.py trans_to_md5 把所有传入的内容转换成 md5，有一个生成验证码的函数，不用管。  

views.py   


#### post 应用  

前端展示的，几乎所有方法都有缓存功能，可以指定过期时间，因为数据要每天更新，所以可以指定时间过期  

ZKYCache 类，操控 Redis，缓存是用 Redis 实现的  

get_by_args 函数，根据传的参数取 name  

HotPostsView 就是 5 篇文章，每天更新，先尝试获取缓存，  

important_search 是重点资讯  


### 项目总体介绍  

#### connect 包

各种连接方式、ES 还多一个分页方法，这个方法比较重要，用的比较多。  

kafka 有一个生产和消费，send 生产，receive 是消费。get_consumer 指定消费者  

MySQL 就一个连接  


#### filters 包

全是匹配部分  

base.py 传一组规则，传入规则名，同义词、限定词、反向限定词，主要是查国家用的。高级搜索里使用。key_rules 用于匹配，可以同时匹配多条规则。  

add_key 一组规则，add_keys 多组规则。exec_filter 是查询用的，传入规则  

reverse_keyfilter_result()，把结果转换了一种返回形式。    

esm 传很多关键词，匹配一句话就能把命中的关键词和位置筛出来，在 index 的 AutoIndex 类中。  


#### tools 包

各种工具  

mx_simhash.py 不看  

text_tools.py 扩展的文本处理，有各种各样的处理方法，可以链式调用（返回了 self）（有很大的改进空间）  

threadingadapter.py 多线程，多进程，返回结果队列传个 piplines 中使用    

utils.py 延迟加载，不重要  


#### piplines 包

normalize_data 格式化数据  

process_data 处理数据  


接收数据、处理数据、返回数据，异步运行（多线程，多进程）  

linemodule.py last_step_queue 接收 threadingadapter.py 中处理完成的数据，主要有 3 个方法，read_data 读数据，normalize_data 预处理，process_data 是正规处理数据。  

db_read_module.py 从数据库批量读数据，返回数据  

es_bulk_module.py 批量操作 ES，主要是增删改    

es_craw_module.py 主要是从 ES 中查数据   

forecast_module.py 筛选包好预测关键词的文章，去除不在的  

lac_module.py 提取人名、机构、关键词，专家观点。给产品进行规则分析用的。  

origin_post_module.py 没用，不看  

output_module.py 转换成 Excel 格式。不用看  

zhili_module.py  选取有治理关键词的文章  



#### user 应用

下 utils.py trans_to_md5 把所有传入的内容转换成 md5，有一个生成验证码的函数，不用管。  

views.py   


#### post 应用  

前端展示的，几乎所有方法都有缓存功能，可以指定过期时间，因为数据要每天更新，所以可以指定时间过期  

ZKYCache 类，操控 Redis，缓存是用 Redis 实现的  

get_by_args 函数，根据传的参数取 name  

HotPostsView 就是 5 篇文章，每天更新，先尝试获取缓存，  

important_search 是重点资讯  


