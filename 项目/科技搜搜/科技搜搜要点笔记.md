
## 单点突破  

### upload_history_v2.py  

这个命令非常重要，有提纲挈领的作用。先读懂主程序，再读各自的具体实现。  

#### 参数 add_arguments  

i: index，目的索引  
pi： point index 专家观点目的索引  
tp: type 类型  
file: 数据规则配置文件  
si: search index 查询数据的索引，就是数据源  
us: use_sentence_cache 句子缓存所用的名字  
uc: use_title_cache 标题缓存所用的名字  

#### handle 函数  





### 重点资讯  

1、重点资讯 important_search_middleware.py 中的 es 函数里，flag 代表是监管部门，还是中央媒体。  

2、CACHE_DEBUG 的重点是 debug，如果为 True，就代表正在进行 debug，不要使用 cache，一般就是设置成 False。  

如果 CACHE_DEBUG 为 False，那么在 cache_worker.py 中的 settings.CACHE_DEBUG 也是 False，这样的话就会去 Redis 里真的查缓存存在不存在。  

如果 CACHE_DEBUG 为 True，那么在 cache_worker.py 中的 settings.CACHE_DEBUG 也是 True，这样就会走 if，这样就会一直返回 False，这样只要调用 exists 方法就会返回 False，这样就相当于没有缓存。  

3、search_mode 是从数据库查还是从 ES 查  

4、
search_args_name: 'search_args_namef93a0f9e4ba9a551fcaaf8d5b26dfd01'  
search_name: 'important_posts2e80ecb46789a80f8e25551cf0952827'  

cache_keywords_name: 'search_args_namef93a0f9e4ba9a551fcaaf8d5b26dfd01'  # 关键词
all_posts_cache_name: 'important_posts8c02f9925c558a34ef62023be502d113'   # 关键词、时间区间、flag 
search_after_cache_name: 'important_search_after_1da8cebefee1aef4c637fb332af1b085'  
title_hash_cache_name: 'important_title_hash_1da8cebefee1aef4c637fb332af1b085'  
important_count_cache_name: 'important_count_1da8cebefee1aef4c637fb332af1b085'  

5、views.py 中的 lambda x: x 是什么意思？filter 必须要传一个 function。  

6、es_query_dict 是怎么来的？是从缓存里取出来的。怎么存进去的？   

7、title_hash 在 update_history_data_to_zky_es.py 中，也就是说，在上数据的时候就已经做了处理了。  

8、all_posts_cache_name 也不是 all 了，是这一批的 posts  




### 国家文章列表  

1、flag 用来区分是 title_country、text_country 还是全部。  
flag = 1，就是 title  
flag = 2，就是 text  
否则就是全部  


2、trans_posts_to_web_format 函数  

把 country tag 替换掉，添加名词高亮显示。  


3、按标题去重是在 rpush 里完成的，在 rpush 函数里调用了 LastUpdatedOrderedDict，这个类用 \_\_setitem__ 实现了覆盖。  

也就是说存到缓存里的已经是去重以后的内容了，然后返回的值，就是从缓存里取的，所以返回的值就是去重以后的值。所以即使已经设置 CACHE_DEBUG = True 了，缓存里还是会有东西。  








#### 环境配置  

1、测试环境  

测试环境的配置在 settings_test.py 中。  

数据库是：192.168.241.51  
ES 索引：kejisousou-test  
Redis 缓存：192.168.241.20，数据在 4 库里  


2、正式环境  

正式环境的配置在 settings.py 中  

数据库是：192.168.241.23  
ES 索引：test-zky  
Redis 缓存：192.168.241.20，数据在 7 库里  



### 项目总体介绍  

#### connect 包

各种连接方式、ES 还多一个分页方法，这个方法比较重要，用的比较多。  

kafka 有一个生产和消费，send 生产，receive 是消费。get_consumer 指定消费者  

MySQL 就一个连接  


#### filters 包

全是匹配部分  

base.py 传一组规则，传入规则名，同义词、限定词、反向限定词，主要是查国家用的。高级搜索里使用。key_rules 用于匹配，可以同时匹配多条规则。  

add_key 一组规则，add_keys 多组规则。exec_filter 是查询用的，传入规则  

reverse_keyfilter_result()，把结果转换了一种返回形式。    

esm 传很多关键词，匹配一句话就能把命中的关键词和位置筛出来，在 index 的 AutoIndex 类中。  


#### tools 包

各种工具  

mx_simhash.py 不看  

text_tools.py 扩展的文本处理，有各种各样的处理方法，可以链式调用（返回了 self）（有很大的改进空间）  

threadingadapter.py 多线程，多进程，返回结果队列传个 piplines 中使用    

utils.py 延迟加载，不重要  


#### piplines 包

normalize_data 格式化数据  

process_data 处理数据  


接收数据、处理数据、返回数据，异步运行（多线程，多进程）  

linemodule.py last_step_queue 接收 threadingadapter.py 中处理完成的数据，主要有 3 个方法，read_data 读数据，normalize_data 预处理，process_data 是正规处理数据。  

db_read_module.py 从数据库批量读数据，返回数据  

es_bulk_module.py 批量操作 ES，主要是增删改    

es_craw_module.py 主要是从 ES 中查数据   

forecast_module.py 筛选包好预测关键词的文章，去除不在的  

lac_module.py 提取人名、机构、关键词，专家观点。给产品进行规则分析用的。  

origin_post_module.py 没用，不看  

output_module.py 转换成 Excel 格式。不用看  

zhili_module.py  选取有治理关键词的文章  



#### user 应用

下 utils.py trans_to_md5 把所有传入的内容转换成 md5，有一个生成验证码的函数，不用管。  

views.py   


#### post 应用  

前端展示的，几乎所有方法都有缓存功能，可以指定过期时间，因为数据要每天更新，所以可以指定时间过期  

ZKYCache 类，操控 Redis，缓存是用 Redis 实现的  

get_by_args 函数，根据传的参数取 name  

HotPostsView 就是 5 篇文章，每天更新，先尝试获取缓存，  

important_search 是重点资讯  



### 数据  

数据库 zky_posts 是中科院的数据表  

upload_to_community 项目，从 kafka 上传数据到 ES  

upload_to_community 文件夹下的 upload.py，上传数据用的，不是查    

normalize_else 是上传到中科院  


XSite 是所有的有效的域名，取到域名以后，会和这个数据库比对，取到有效的域名  

entry 是当前，site 父域名   


