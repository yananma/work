
```python 
    max_group_sentences = []
    for group in groups:    # groups 是所有的分组，group 是同一个位置的所有的文字
        # 差异性最大的是字幕
        if len(set(group['sentences'])) > len(set(max_group_sentences)):
            group["is_zimu"] = True
            max_group_sentences = group['sentences']
        else:
            group["is_zimu"] = False

    update_is_zimu(groups)

    # 最后再做一次过滤
    if len(set(max_group_sentences)) <= 5:
        max_group_sentences = []

    all_zimu_sentences = ','.join(max_group_sentences)
    txt.write(all_zimu_sentences)
    txt.close()
```


```python 
def main(video_name):
    frame_dir = settings.VIDEO_FRAME_DIR + video_name.split('.')[0] + '/'
    output_dir = settings.OUTPUT_DIR + video_name.split('.')[0] + '/'
    if not (os.path.exists(output_dir)):
        os.mkdir(output_dir)
    ocr = PaddleOCR(use_angle_cls=True, lang="ch", use_gpu=False)
    run_time = datetime.datetime.now().strftime("%Y_%m_%d_%H_%M_%S")
    txt = open(output_dir + str(run_time) + '.txt', 'a', encoding='utf-8')
    frames = sorted(filter(is_img, os.listdir(frame_dir)))

    groups = grouping_by_position(frames, frame_dir, ocr)

    max_group_sentences = []
    for group in groups:    # groups 是所有的分组，group 是同一个位置的所有的文字
        # 差异性最大的是字幕
        if len(set(group['sentences'])) > len(set(max_group_sentences)):
            group["is_zimu"] = True
            max_group_sentences = group['sentences']
        else:
            group["is_zimu"] = False

    update_is_zimu(groups)

    # 最后再做一次过滤
    if len(set(max_group_sentences)) <= 5:
        max_group_sentences = []

    all_zimu_sentences = ','.join(max_group_sentences)
    txt.write(all_zimu_sentences)
    txt.close()
    get_zimu_position(groups)
    print('finish.')
``` 

```python
def get_ocr_result(frame_dir, img, ocr):
    """一次传入一张图片，OCR 识别这张图片的所有文字，返回结果为[{},{}]格式"""
    image = frame_dir + img
    result = ocr.ocr(image, cls=True)  # OCR 识别。
    print(result)
    ret_result = []
    # line[1][0], line[1][1], line[0]  # 文字   置信度    位置
    for line in result:
        left_upper_y = line[0][0][1]
        left_bottom_y = line[0][3][1]
        ret_result.append({
            'sentence': line[1][0],
            'txt_position': line[0],
            'top': left_upper_y,
            'height': left_bottom_y - left_upper_y
        })
    return ret_result
```  

ocr.py main()  

```python 
   output_dir = settings.OUTPUT_DIR + video_name.split('.')[0] + '/'
   if not (os.path.exists(output_dir)):
       os.mkdir(output_dir)
```

ocr.py grouping_by_position()  
```python 
                # Avoid duplicate: check if current word is similar to last word
                if len(group['sentences']) >= 3:
                    for last_word in group['sentences'][-3:]:
                        if difflib.SequenceMatcher(None, last_word, sentence).quick_ratio() > 0.8:
                            duplicate_flag = True
                            break
                else:
                    last_word = group['sentences'][len(group['sentences']) - 1]
                    if difflib.SequenceMatcher(None, last_word, sentence).quick_ratio() > 0.8:
                        break
                if duplicate_flag:
                    break
```

ocr.py   
```python 
def separate_zimu_huazi(ocr_result, zimu_top, zimu_height):
    """分离字幕和花字，遍历每一句的识别结果，和字幕位置作比较"""
    zimu_list = []
    huazi_list = []
    if zimu_top is None and zimu_height is None:  # 如果没有字幕，则全部文字都是花字
        for line in ocr_result:
            if line['sentence'] not in huazi_list:
                huazi_list.append(line['sentence'])
        return zimu_list, huazi_list
    for line in ocr_result:
        if abs(line["top"] - zimu_top) <= 3 * zimu_height and abs(line["height"] - zimu_height) < 0.15 * zimu_height:
            zimu_list.append(line['sentence'])
        else:
            huazi_list.append(line['sentence'])
    return zimu_list, huazi_list
``` 

ocr.py  
```python 
def separate_zimu_huazi(ocr_result, zimu_top, zimu_height):
    """分离字幕和花字，遍历每一句的识别结果，和字幕位置作比较"""
    zimu_list = []
    huazi_list = []
    if zimu_top is None and zimu_height is None:  # 如果没有字幕，则全部文字都是花字
        for line in ocr_result:
            if line['sentence'] not in huazi_list:
                huazi_list.append(line['sentence'])
        return zimu_list, huazi_list
    for line in ocr_result:
        duplicate_flag = False
        if abs(line["top"] - zimu_top) <= 3 * zimu_height and abs(line["height"] - zimu_height) < 0.15 * zimu_height:
            if len(zimu_list) >= 3:
                for last_word in zimu_list[-3:]:
                    if difflib.SequenceMatcher(None, last_word, line['sentence']).quick_ratio() > 0.8:
                        duplicate_flag = True
                        continue
            else:
                for last_word in zimu_list:
                    if difflib.SequenceMatcher(None, last_word, line['sentence']).quick_ratio() > 0.8:
                        duplicate_flag = True
                        continue
            if duplicate_flag:
                continue
            zimu_list.append(line['sentence'])
        else:
            if len(huazi_list) >= 3:
                for last_word in huazi_list[-3:]:
                    if difflib.SequenceMatcher(None, last_word, line['sentence']).quick_ratio() > 0.8:
                        duplicate_flag = True
                        continue
            else:
                for last_word in huazi_list:
                    if difflib.SequenceMatcher(None, last_word, line['sentence']).quick_ratio() > 0.8:
                        duplicate_flag = True
                        continue
            if duplicate_flag:
                continue
            huazi_list.append(line['sentence'])
    return zimu_list, huazi_list
```

ocr.py  严格去重，效果更好  
```python 
def separate_zimu_huazi(ocr_result, zimu_top, zimu_height):
    """分离字幕和花字，遍历每一句的识别结果，和字幕位置作比较"""
    zimu_list = []
    huazi_list = []
    if zimu_top is None and zimu_height is None:  # 如果没有字幕，则全部文字都是花字
        for line in ocr_result:
            if line['sentence'] not in huazi_list:
                huazi_list.append(line['sentence'])
        return zimu_list, huazi_list
    for line in ocr_result:
        if abs(line["top"] - zimu_top) <= 3 * zimu_height and abs(line["height"] - zimu_height) < 0.15 * zimu_height:
            if line['sentence'] not in zimu_list:
                zimu_list.append(line['sentence'])
        else:
            if line['sentence'] not in huazi_list:
                huazi_list.append(line['sentence'])
    return zimu_list, huazi_list
``` 

大改前备份    
```python 
# -*- coding:utf-8 -*-
import difflib
import importlib
import json
import sys

import pandas as pd

import settings

importlib.reload(sys)
pd.set_option('display.max_rows', None)
pd.set_option('max_colwidth', 800)


def get_ocr_result(df):
    """读取 CSV 文件，返回整个 video 的 OCR 识别结果"""
    df = df.sort_values(by=[0, 3], key=lambda x: x if x.name == 0 else x.map(lambda x: json.loads(x)[0][1]))  # 排序，先按图片名排，同一张图片中按文字从上往下排
    ret_result = []
    for row in df.iterrows():
        position = json.loads(row[1][3])
        left_upper_y = position[0][1]
        left_bottom_y = position[3][1]
        ret_result.append({
            'sentence': row[1][2],
            'txt_position': position,
            'top': left_upper_y,
            'height': left_bottom_y - left_upper_y
        })
    return ret_result  # 返回结果为 [{},{},{}...] 格式，字典中为每一句的文本和位置


def update_is_zimu(groups):
    """
    修正 is_zimu 字段。
    因为之前的判断，一个 video 里可能会有好几个 is_zimu 为 True 的 group。这里修正以后，只有真正的字幕的 is_group 为 True
    """
    find_zimu = False
    for group in reversed(groups):
        if find_zimu == False and group["is_zimu"] == True:
            find_zimu = True
        else:
            group["is_zimu"] = False


def add_is_zimu(groups):
    """给字幕分组添加 is_zimu 字段"""
    max_sent_list = []
    for group in groups:  # groups 是所有的分组，group 是同一个位置的所有的文字
        # 差异性最大的是字幕
        if len(set(group['sentences'])) > len(set(max_sent_list)):
            group["is_zimu"] = True
            max_sent_list = group['sentences']
        else:
            group["is_zimu"] = False
    update_is_zimu(groups)
    # 最后再做一次过滤
    if len(set(max_sent_list)) <= 4:
        max_sent_list = []
    return max_sent_list


def get_zimu_position(groups):
    """获取字幕的位置，也就是字幕的 top 和 height"""
    max_group_sentences = add_is_zimu(groups)
    if not max_group_sentences:  # 如果 max_group_sentences 为空，也就是最大组去重以后句子个数小于等于 5 个的，就判定为没有字幕，即全部都是花字
        for group in groups:
            print(group)
        return None, None
    for group in groups:
        if group["is_zimu"]:
            print()
            print(group)
            # return group["top"], group["height"]
        else:
            print()
            print(group)
    for group in groups:
        if group["is_zimu"]:
            return group["top"], group["height"]


def grouping_by_position(df):
    """按位置分组，是整个程序的核心"""
    groups = []
    ocr_result = get_ocr_result(df)
    # print(ocr_result)
    for line in ocr_result:  # ocr_result 是同一张图片的所有文字的识别结果
        top = int(line['top'])
        height = int(line['height'])
        sentence = line['sentence']
        exist = False
        for group in groups:
            duplicate_flag = False
            # 遍历已有识别结果，如果新的结果的 top 值与 group 的 top 之差小于区域高度的 2/3，而且高度之差不超过原有 height 的 15%，则认为是一组的
            if abs(group['top'] - top) < (group['height'] / 3 * 2) and abs(group["height"] - height) < 0.15 * group["height"]:
                exist = True  # 这个位置的分组已经存在
                # Avoid duplicate: check if current word is similar to last word
                if len(group['sentences']) >= 3:
                    for last_word in group['sentences'][-3:]:
                        if difflib.SequenceMatcher(None, last_word, sentence).quick_ratio() > 0.8:
                            duplicate_flag = True
                            break
                else:
                    for last_word in group['sentences']:
                        if difflib.SequenceMatcher(None, last_word, sentence).quick_ratio() > 0.8:
                            duplicate_flag = True
                            break
                if duplicate_flag:
                    break
                group['sentences'].append(sentence)
                # update group position
                group['total_top'] += top
                group['total_height'] += height
                group['total_num'] += 1
                group['top'] = int(group['total_top'] / group['total_num'])
                group['height'] = int(group['total_height'] / group['total_num'])
                break

        if not exist:  # 如果这个位置的分组不存在，就创建这个位置的分组
            groups.append({
                'top': top,  # group standard, using average value of tops
                'total_top': top,
                'height': height,
                'total_height': height,
                'total_num': 1,  # how many pics has been add to this group
                'sentences': [sentence]
            })
    return groups, ocr_result


def separate_zimu_huazi(ocr_result, zimu_top, zimu_height):
    """分离字幕和花字，遍历每一句的识别结果，和字幕位置作比较"""
    zimu_list = []
    huazi_list = []
    if zimu_top is None and zimu_height is None:  # 如果没有字幕，则全部文字都是花字
        for line in ocr_result:
            if line['sentence'] not in huazi_list:
                huazi_list.append(line['sentence'])
        return zimu_list, huazi_list
    for line in ocr_result:
        duplicate_flag = False
        if abs(line["top"] - zimu_top) <= 3 * zimu_height and abs(line["height"] - zimu_height) <= 0.25 * zimu_height:
            # if line['sentence'] not in zimu_list:
            #     zimu_list.append(line['sentence'])
            if len(zimu_list) >= 3:
                for last_word in zimu_list[-3:]:
                    if difflib.SequenceMatcher(None, last_word, line['sentence']).quick_ratio() > 0.8:
                        duplicate_flag = True
                        continue
            else:
                for last_word in zimu_list:
                    if difflib.SequenceMatcher(None, last_word, line['sentence']).quick_ratio() > 0.8:
                        duplicate_flag = True
                        continue
            if duplicate_flag:
                continue
            zimu_list.append(line['sentence'])
        else:
            if line['sentence'] not in huazi_list:
                huazi_list.append(line['sentence'])
    return zimu_list, huazi_list


def main(video_name):
    df = pd.read_csv(settings.CSV_DIR + video_name.split(".")[0] + '.csv', header=None)
    groups, ocr_result = grouping_by_position(df)
    zimu_top, zimu_height = get_zimu_position(groups)
    zimu_list, huazi_list = separate_zimu_huazi(ocr_result, zimu_top, zimu_height)
    print()
    print('字幕')
    print(",".join(zimu_list))
    print()
    print('花字')
    print(",".join(huazi_list))
    print('finish.')
``` 

```python  
```  
大改前备份，修改为类，并实现按帧去重 02.27    

run.py    
```python 
import os
import get_frames
import ocr
import save_to_csv


# video_name = "huazi" + ".mp4"   # 一张一张规则
# video_name = "huazi1" + ".mp4"   # 稳定字幕
# video_name = "huazi2" + ".mp4"   # 包装袋图片
# video_name = "single_zimu" + ".mp4"   # 金毛狗
# video_name = "single_zimu2" + ".mp4"   # 稳定字幕
# video_name = "jinmao" + ".mp4"   # 金毛狗
# video_name = "multi_zimu1" + ".mp4"   # 北京冬奥会闭幕式
# video_name = "multi_zimu2" + ".mp4"   # 北京冬奥会闭幕式
# video_name = "multi_zimu3" + ".mp4"   # 兽医


# 测试
# video_name = "test1" + ".mp4"
# video_name = "test2" + ".mp4"


# dy
# video_name = "dy1" + ".mp4"
# video_name = "dy2" + ".mp4"
# video_name = "dy3" + ".mp4"
# video_name = "dy4" + ".mp4"


# get_frames.main(video_name)
# save_to_csv.main(video_name)
# ocr.main(video_name)

# douyin_video_list = [video for video in os.listdir('/home/test/syb/mayanan/nielsen/video') if video.startswith('douyin')]

# for video_name in douyin_video_list:
    # video_name = video_name.split('/')[-1]
    # get_frames.main(video_name)
    # save_to_csv.main(video_name)
    # ocr.main(video_name)


douyin_video_list = [video for video in os.listdir('/home/crisis/nielsen/video/dy1/')]

for video_name in douyin_video_list:
    video_name = video_name.split('/')[-1]
    # print(video_name)
    # get_frames.main(video_name)
    # save_to_csv.main(video_name)
    ocr.main(video_name)
```

get_frames.py   
```python 
# -*- coding:utf-8 -*-
# 视频切帧
import os

import cv2

import settings
import save_to_csv

import logging
from importlib import reload
reload(logging)
logging.basicConfig(format='%(asctime)s %(levelname)s:%(message)s', level=logging.DEBUG, datefmt='%I:%M:%S')
logger = logging.getLogger()
logger.setLevel(logging.DEBUG)


def main(video_name):
    video_file = settings.VIDEO_FILE_DIR + video_name
    frame_dir = settings.VIDEO_FRAME_DIR + video_name.split(".")[0] + '/'

    if not (os.path.exists(frame_dir)):
        os.mkdir(frame_dir)

    vid_cap = cv2.VideoCapture(video_file)  # 读入视频文件

    if vid_cap.isOpened():  # 判断是否正常打开
        success, frame = vid_cap.read()
    else:
        success = False

    time_interval = int(vid_cap.get(5) * settings.SPLIT_DURATION)  # 视频帧计数间隔频率 = 帧率 * 切片时间间隔

    count = 0
    img_count = 0
    while success:  # 循环读取视频帧
        success, frame = vid_cap.read()
        if count % time_interval == 0:  # 每隔timeF帧进行存储操作
            logger.info(f'切第 {img_count} 帧')
            try:
                cv2.imencode('.jpg', frame)[1].tofile(frame_dir + str(img_count).zfill(10) + '.jpg')  # 存储为图像
                img_count += 1
            except cv2.error:
                pass
        count = count + 1
        cv2.waitKey(1)
    vid_cap.release()
    # save_to_csv.main(video_name)
``` 

save_to_csv.py  

```python 
import json
import os
import re

import pandas as pd
from paddleocr import PaddleOCR

import settings


def get_img_ocr_result(frame_dir, img, ocr):
    """一次传入一张图片，OCR 识别这张图片的所有文字，并返回特定的格式"""
    image = frame_dir + img
    result = ocr.ocr(image, cls=True)  # OCR 识别一张图片，result 为一张图片的结果。
    one_img_ret_result = []
    for line in result:
        confidence = line[1][1]
        text = line[1][0]
        position = json.dumps(line[0])
        one_img_ret_result.append([img, confidence, text, position])
    # print(one_img_ret_result)
    return one_img_ret_result


def is_img(f):
    return re.match(r'.+jpg', f)


def get_video_ocr_result(video_name):
    """返回整个视频的结果"""
    frame_dir = settings.VIDEO_FRAME_DIR + video_name.split('.')[0] + '/'
    ocr = PaddleOCR(use_angle_cls=True, lang="ch", use_gpu=True)
    frames = sorted(filter(is_img, os.listdir(frame_dir)))
    final_list = []
    for frame in frames:
        one_img_result = get_img_ocr_result(frame_dir, frame, ocr)
        final_list.extend(one_img_result)
    return final_list


def main(video_name):
    final_list = get_video_ocr_result(video_name)
    df = pd.DataFrame(final_list)
    df.to_csv(settings.CSV_DIR + video_name.split('.')[0] + '.csv', index=False, header=False)
    print("finish save_to_csv.")
```   

ocr.py   

```python 
# -*- coding:utf-8 -*-
import difflib
import importlib
import json
import sys

import pandas as pd

import settings

importlib.reload(sys)


def get_ocr_result(df):
    """读取 CSV 文件，返回整个 video 的 OCR 识别结果"""
    df = df.sort_values(by=[0, 3], key=lambda x: x if x.name == 0 else x.map(lambda x: json.loads(x)[0][1]))  # 排序，先按图片名排，同一张图片中按文字从上往下排
    df = df.dropna(axis=0, how='any')  # 删除包含 nan 的行
    ret_result = []
    for row in df.iterrows():
        position = json.loads(row[1][3])
        left_upper_y = position[0][1]
        left_bottom_y = position[3][1]
        ret_result.append({
            'sentence': row[1][2],
            'txt_position': position,
            'top': left_upper_y,
            'height': left_bottom_y - left_upper_y
        })
    return ret_result  # 返回结果为 [{},{},{}...] 格式，字典中为每一句的文本和位置


def update_is_zimu(groups):
    """
    修正 is_zimu 字段。
    因为之前的判断，一个 video 里可能会有好几个 is_zimu 为 True 的 group。这里修正以后，只有真正的字幕的 is_group 为 True
    """
    find_zimu = False
    for group in reversed(groups):
        if find_zimu == False and group["is_zimu"] == True:
            find_zimu = True
        else:
            group["is_zimu"] = False


def add_is_zimu(groups):
    """给字幕分组添加 is_zimu 字段"""
    max_sent_list = []
    for group in groups:  # groups 是所有的分组，group 是同一个位置的所有的文字
        # 差异性最大的是字幕
        if len(set(group['sentences'])) > len(set(max_sent_list)):
            group["is_zimu"] = True
            max_sent_list = group['sentences']
        else:
            group["is_zimu"] = False
    update_is_zimu(groups)
    # 最后再做一次过滤
    if len(set(max_sent_list)) <= 4:
        max_sent_list = []
    return max_sent_list


def get_zimu_position(groups):
    """获取字幕的位置，也就是字幕的 top 和 height"""
    max_group_sentences = add_is_zimu(groups)
    if not max_group_sentences:  # 如果 max_group_sentences 为空，也就是最大组去重以后句子个数小于等于 5 个的，就判定为没有字幕，即全部都是花字
        # for group in groups:
        #     print(group)
        return None, None
    # for group in groups:
    #     if group["is_zimu"]:
    #         print()
    #         print(group)
    #         # return group["top"], group["height"]
    #     else:
    #         print()
    #         print(group)
    for group in groups:
        if group["is_zimu"]:
            return group["top"], group["height"]


def grouping_by_position(df):
    """按位置分组，是整个程序的核心"""
    groups = []
    ocr_result = get_ocr_result(df)
    # print(ocr_result)
    for line in ocr_result:  # ocr_result 是同一张图片的所有文字的识别结果
        top = int(line['top'])
        height = int(line['height'])
        sentence = line['sentence']
        exist = False
        for group in groups:
            duplicate_flag = False
            # 遍历已有识别结果，如果新的结果的 top 值与 group 的 top 之差小于区域高度的 2/3，而且高度之差不超过原有 height 的 15%，则认为是一组的
            if abs(group['top'] - top) < (group['height'] / 3 * 2) and abs(group["height"] - height) < 0.15 * group["height"]:
                exist = True  # 这个位置的分组已经存在
                # Avoid duplicate: check if current word is similar to last word
                if len(group['sentences']) >= 3:
                    for last_word in group['sentences'][-3:]:
                        if difflib.SequenceMatcher(None, last_word, sentence).quick_ratio() > 0.8:
                            duplicate_flag = True
                            break
                else:
                    for last_word in group['sentences']:
                        if difflib.SequenceMatcher(None, last_word, sentence).quick_ratio() > 0.8:
                            duplicate_flag = True
                            break
                if duplicate_flag:
                    break
                group['sentences'].append(sentence)
                # update group position
                group['total_top'] += top
                group['total_height'] += height
                group['total_num'] += 1
                group['top'] = int(group['total_top'] / group['total_num'])
                group['height'] = int(group['total_height'] / group['total_num'])
                break

        if not exist:  # 如果这个位置的分组不存在，就创建这个位置的分组
            groups.append({
                'top': top,  # group standard, using average value of tops
                'total_top': top,
                'height': height,
                'total_height': height,
                'total_num': 1,  # how many pics has been add to this group
                'sentences': [sentence]
            })
    return groups, ocr_result


def separate_zimu_huazi(ocr_result, zimu_top, zimu_height):
    """分离字幕和花字，遍历每一句的识别结果，和字幕位置作比较"""
    zimu_list = []
    huazi_list = []
    if zimu_top is None and zimu_height is None:  # 如果没有字幕，则全部文字都是花字
        for line in ocr_result:
            if line['sentence'] not in huazi_list:
                huazi_list.append(line['sentence'])
        return zimu_list, huazi_list
    for line in ocr_result:
        duplicate_flag = False
        if abs(line["top"] - zimu_top) <= 3 * zimu_height and abs(line["height"] - zimu_height) <= 0.25 * zimu_height:
            # if line['sentence'] not in zimu_list:
            #     zimu_list.append(line['sentence'])
            if len(zimu_list) >= 3:
                for last_word in zimu_list[-3:]:
                    if difflib.SequenceMatcher(None, last_word, line['sentence']).quick_ratio() > 0.8:
                        duplicate_flag = True
                        continue
            else:
                for last_word in zimu_list:
                    if difflib.SequenceMatcher(None, last_word, line['sentence']).quick_ratio() > 0.8:
                        duplicate_flag = True
                        continue
            if duplicate_flag:
                continue
            zimu_list.append(line['sentence'])
        else:
            if line['sentence'] not in huazi_list:
                huazi_list.append(line['sentence'])
    return zimu_list, huazi_list


def main(video_name):
    df = pd.read_csv(settings.CSV_DIR + video_name.split(".")[0] + '.csv', header=None)
    groups, ocr_result = grouping_by_position(df)
    zimu_top, zimu_height = get_zimu_position(groups)
    zimu_list, huazi_list = separate_zimu_huazi(ocr_result, zimu_top, zimu_height)
    print()
    print(video_name)
    print(f'字幕：{",".join(zimu_list)}')
    print()
    print(f'花字：{",".join(huazi_list)}')
    print('finish.')
```

```python  
```  


备份  全部用时间去重，效果不好，比之前多了几百行，速度也非常慢 ocr.py   

```python 
# -*- coding:utf-8 -*-
import difflib
import importlib
import json
import sys

import pandas as pd

import settings

importlib.reload(sys)


class OCRResult:
    """OCR 识别结果，包含结果的各种额外信息"""
    def __init__(self, name, text, position, is_zimu=False):
        self.name = name  # csv 第一列，也就是文件名和帧数
        self.frame = self.name.split('_')[1].split('.')[0]
        self.frame_time = int(self.frame) * settings.SPLIT_DURATION
        self.sentence = text
        if isinstance(position, str):
            self.position = json.loads(position)
        elif isinstance(position, list):
            self.position = position
        else:
            raise AttributeError("position 格式不对")
        self.left_upper, self.right_upper, self.right_bottom, self.left_bottom = self.position
        self.top = self.left_upper[1]
        self.height = self.left_bottom[1] - self.left_upper[1]
        self.is_zimu = is_zimu

    def __str__(self):
        return self.sentence

    def __repr__(self):
        return f'sentence: {self.sentence} top: {self.top} height: {self.height} ' \
               f'is_zimu: {self.is_zimu} frame: {self.frame} frame_time: {self.frame_time}'


def get_ocr_result(df):
    """读取 CSV 文件，返回整个 video 的 OCR 识别结果"""
    df = df.sort_values(by=[0, 3], key=lambda x: x if x.name == 0 else x.map(lambda x: json.loads(x)[0][1]))  # 排序，先按图片名排，同一张图片中按文字从上往下排
    df = df.dropna(axis=0, how='any')  # 删除包含 nan 的行
    ret_result = []
    for row in df.iterrows():
        row = row[1]
        name, text, position = row[0], row[2], row[3]
        one_ocr_result = OCRResult(name, text, position)
        ret_result.append(one_ocr_result)
    return ret_result


def update_is_zimu(groups):
    """
    修正 is_zimu 字段。
    因为之前的判断，一个 video 里可能会有好几个 is_zimu 为 True 的 group。这里修正以后，只有真正的字幕的 is_group 为 True
    """
    find_zimu = False
    for group in reversed(groups):
        if find_zimu == False and group["is_zimu"] == True:
            find_zimu = True
        else:
            group["is_zimu"] = False


def add_is_zimu(groups):
    """给字幕分组添加 is_zimu 字段"""
    max_sent_list = []
    for group in groups:  # groups 是所有的分组，group 是同一个位置的所有的文字
        # 差异性最大的是字幕
        if len(set(group['sentences'])) > len(set(max_sent_list)):
            group["is_zimu"] = True
            max_sent_list = group['sentences']
        else:
            group["is_zimu"] = False
    update_is_zimu(groups)
    # 最后再做一次过滤
    if len(set(max_sent_list)) <= 4:
        max_sent_list = []
    return max_sent_list


def get_zimu_position(groups):
    """获取字幕的位置，也就是字幕的 top 和 height"""
    max_group_sentences = add_is_zimu(groups)
    if not max_group_sentences:  # 如果 max_group_sentences 为空，也就是最大组去重以后句子个数小于等于 5 个的，就判定为没有字幕，即全部都是花字
        # for group in groups:
        #     print(group)
        return None, None
    # for group in groups:
    #     if group["is_zimu"]:
    #         print()
    #         print(group)
    #         # return group["top"], group["height"]
    #     else:
    #         print()
    #         print(group)
    for group in groups:
        if group["is_zimu"]:
            return group["top"], group["height"]


def grouping_by_position(df):
    """按位置分组，是整个程序的核心"""
    groups = []
    ocr_result = get_ocr_result(df)
    # print(ocr_result)
    for one_ocr_result in ocr_result:  # ocr_result 是同一个 video 的所有文字的识别结果，one_ocr_result 是一句
        top = int(one_ocr_result.top)
        height = int(one_ocr_result.height)
        sentence = one_ocr_result.sentence
        exist = False
        for group in groups:
            duplicate_flag = False
            # 遍历已有识别结果，如果新的结果的 top 值与 group 的 top 之差小于区域高度的 2/3，而且高度之差不超过原有 height 的 15%，则认为是一组的
            if abs(group['top'] - top) < (group['height'] / 3 * 2) and abs(group["height"] - height) < 0.15 * group["height"]:
                exist = True  # 这个位置的分组已经存在
                # Avoid duplicate: check if current word is similar to last word
                if len(group['sentences']) >= 3:
                    for last_word in group['sentences'][-3:]:
                        if difflib.SequenceMatcher(None, last_word, sentence).quick_ratio() > 0.8:
                            duplicate_flag = True
                            break
                else:
                    for last_word in group['sentences']:
                        if difflib.SequenceMatcher(None, last_word, sentence).quick_ratio() > 0.8:
                            duplicate_flag = True
                            break
                if duplicate_flag:
                    break
                group['sentences'].append(sentence)
                # update group position
                group['total_top'] += top
                group['total_height'] += height
                group['total_num'] += 1
                group['top'] = int(group['total_top'] / group['total_num'])
                group['height'] = int(group['total_height'] / group['total_num'])
                break

        if not exist:  # 如果这个位置的分组不存在，就创建这个位置的分组
            groups.append({
                'top': top,  # group standard, using average value of tops
                'total_top': top,
                'height': height,
                'total_height': height,
                'total_num': 1,  # how many pics has been add to this group
                'sentences': [sentence]
            })
    return groups, ocr_result


def separate_zimu_huazi(ocr_result, zimu_top, zimu_height):
    """分离字幕和花字，遍历每一句的识别结果，和字幕位置作比较"""
    zimu_list = []
    huazi_list = []
    if zimu_top is None and zimu_height is None:  # 如果没有字幕，则全部文字都是花字
        for one_ocr_result in ocr_result:
            duplicate_flag = False
            if one_ocr_result.frame_time >= 3:
                for last_word in filter(lambda x: x.frame_time > (one_ocr_result.frame_time - 3), huazi_list):
                    if difflib.SequenceMatcher(None, last_word.sentence, one_ocr_result.sentence).quick_ratio() > 0.8:
                        duplicate_flag = True
                        break
            else:
                for last_word in huazi_list:
                    if difflib.SequenceMatcher(None, last_word.sentence, one_ocr_result.sentence).quick_ratio() > 0.8:
                        duplicate_flag = True
                        break
            if duplicate_flag:
                continue
            huazi_list.append(one_ocr_result)
        return zimu_list, huazi_list
    for one_ocr_result in ocr_result:
        duplicate_flag = False
        if abs(one_ocr_result.top - zimu_top) <= 3 * zimu_height and abs(one_ocr_result.height - zimu_height) <= 0.25 * zimu_height:
            if one_ocr_result.frame_time >= 3:
                for last_word in filter(lambda x: x.frame_time > (one_ocr_result.frame_time - 3), zimu_list):
                    if difflib.SequenceMatcher(None, last_word.sentence, one_ocr_result.sentence).quick_ratio() > 0.8:
                        duplicate_flag = True
                        continue
            else:
                for last_word in zimu_list:
                    if difflib.SequenceMatcher(None, last_word.sentence, one_ocr_result.sentence).quick_ratio() > 0.8:
                        duplicate_flag = True
                        continue
            if duplicate_flag:
                continue
            zimu_list.append(one_ocr_result)
        else:
            if one_ocr_result.frame_time >= 3:
                for last_word in filter(lambda x: x.frame_time > (one_ocr_result.frame_time - 3), huazi_list):
                    if difflib.SequenceMatcher(None, last_word.sentence, one_ocr_result.sentence).quick_ratio() > 0.8:
                        duplicate_flag = True
                        continue
            else:
                for last_word in huazi_list:
                    if difflib.SequenceMatcher(None, last_word.sentence, one_ocr_result.sentence).quick_ratio() > 0.8:
                        duplicate_flag = True
                        continue
            if duplicate_flag:
                continue
            huazi_list.append(one_ocr_result)
    return zimu_list, huazi_list


def main(video_name):
    df = pd.read_csv(settings.CSV_DIR + video_name.split(".")[0] + '.csv', header=None)
    groups, ocr_result = grouping_by_position(df)
    zimu_top, zimu_height = get_zimu_position(groups)
    zimu_list, huazi_list = separate_zimu_huazi(ocr_result, zimu_top, zimu_height)
    print()
    print(video_name)
    zimu_sentence_list = [item.sentence for item in zimu_list]
    print(f'字幕：{",".join(zimu_sentence_list)}')
    print()
    huazi_sentence_list = [item.sentence for item in huazi_list]
    print(f'花字：{",".join(huazi_sentence_list)}')
```

most_common 备份  

```python 
def main(video_name):
    df = pd.read_csv(settings.CSV_DIR + video_name.split(".")[0] + '.csv', header=None)
    groups, ocr_result = grouping_by_position(df)
    zimu_top, zimu_height = get_zimu_position(groups)
    zimu_list, huazi_list = separate_zimu_huazi(ocr_result, zimu_top, zimu_height)
    print()
    # print(video_name)
    zimu_sentence_list = [item.sentence for item in zimu_list]
    zimu_sentence_count = Counter(zimu_sentence_list)
    if zimu_sentence_list and zimu_sentence_count.most_common(1)[0][1] >= 3:
        print(video_name)
        print(f'字幕：{",".join(zimu_sentence_list)}')
        print()
        print(f"most common {zimu_sentence_count.most_common(3)}")
    # print()
    # huazi_sentence_list = [item.sentence for item in huazi_list]
    # print(f'花字：{",".join(huazi_sentence_list)}')
``` 

