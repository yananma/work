
v6 是单句训练集  
v7 是单句预测多句训练集  

训练模型命令：`nohup ./run_yuche2.sh train bert trainv3 norm semeval2_b128_r1_lr2e5 1 3 2e-5 1,1 &> logs/mxbert_20211107.log &`  

模型训练 GPU 命令：`./run_yuche2.sh train bert trainv3 norm semeval2_b128_r1_lr2e5 0 3 2e-5 1,1`  

预测命令：`./run_predict2.sh bert model/norm_epoch_3(模型)/ test/`    





### 说明  

[在线文档](https://qrfmglwxn4.feishu.cn/docs/doccnhv1EpDdAcaYTv1F1Vbp1fc)  

是一种在线服务的形式  

SOC 正负面识别模型，用的是词性匹配  
    优点：快  
    缺点：业务不太匹配  
         准确度不够  

用的是历史标注数据，数据量在 1 万条左右，但是是整篇文章的标注，不是句子的标注，这个可以改进  

用的和科技搜搜的预测模型是一致的，不过只用了二分类模型，没有用提取模型  

运行中是根据关键词截取句子；一篇文章只有一个最后的结果。  

问题：  
主要就是速度问题  
1. 统计现在速度是多少 
2. 统计换成 bert 模型之后是多少  
3. 部署到 GPU 以后速度应该会提高不少 
4. SOC 的包可能还有问题  




