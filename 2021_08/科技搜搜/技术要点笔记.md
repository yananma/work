
进入虚拟环境  
cd ~  
dofspy37venv  

handle 方法  

先 delete 全部，就是清除以前的内容  

```python
if not obj.strip():
    continue
```
意思就是如果 obj 不存在就跳过这一个  


zj 是专家  

j 是监管  




### 数据  

数据库 zky_posts 是中科院的数据表  

upload_to_community 项目，从 kafka 上传数据到 ES  

upload_to_community 文件夹下的 upload.py，上传数据用的，不是查    

normalize_else 是上传到中科院  


XSite 是所有的有效的域名，取到域名以后，会和这个数据库比对，取到有效的域名  

entry 是当前，site 父域名   



### ElasticSearch  

ES 遇到问题自己用 kibana 写代码试   

\_index 当表名  
\_source 是 data  
\_id 是主键  
\_type  


非关系型   

restful 风格  

`GET test-zky/_count`  

```python 
GET test-zky/_search
{

}
```

query 是查询条件  

must 是必须满足，有点像 and 关系  


选中执行  

一般数据在 hits 里  

索引相当于表  


data_normalize.py  

zky-all 是备份  

5 种筛选数据的规则，满足任意一条就可以通过    
作者、域名、media 板块、child_map 域名后边带东西 比如 `finance.sina.com.cn/tech`，看 url 是否在目标的 url 里，在就返回 True，不在就返回 False，用 in 判断、site_name 和作者名都要符合的，用 and 关系    


一个 data 就是一条 kafka 数据  

helps 是 ES 的库，helpers.bulk 是批量操作，比如批量插入，如果数据重复就报错，一般忽略重复，不做重复判断    


flush_else() 把数据上传到 ES 以后删除内存里的数据  






