
进入虚拟环境  
cd ~  
dofspy37venv  

handle 方法  

先 delete 全部，就是清除以前的内容  

```python
if not obj.strip():
    continue
```
意思就是如果 obj 不存在就跳过这一个  


zj 是专家  

j 是监管  




### 数据  

数据库 zky_posts 是中科院的数据表  

upload_to_community 项目，从 kafka 上传数据到 ES  

upload_to_community 文件夹下的 upload.py，上传数据用的，不是查    

normalize_else 是上传到中科院  


XSite 是所有的有效的域名，取到域名以后，会和这个数据库比对，取到有效的域名  

entry 是当前，site 父域名   



### ElasticSearch  

ES 遇到问题自己用 kibana 写代码试   

\_index 当数据库  
\_type 当做表  
\_source 是 data  
\_document 是行  
\_fields 是字段  
\_id 是主键  


非关系型   

restful 风格  

`GET test-zky/_count`  

```python 
GET test-zky/_search
{

}
```

query 是查询条件  

must 是必须满足，有点像 and 关系  


选中执行  

一般数据在 hits 里  

索引相当于表  


data_normalize.py  

zky-all 是备份  

5 种筛选数据的规则，满足任意一条就可以通过    
作者、域名、media 板块、child_map 域名后边带东西 比如 `finance.sina.com.cn/tech`，看 url 是否在目标的 url 里，在就返回 True，不在就返回 False，用 in 判断、site_name 和作者名都要符合的，用 and 关系    


一个 data 就是一条 kafka 数据  

helps 是 ES 的库，helpers.bulk 是批量操作，比如批量插入，如果数据重复就报错，一般忽略重复，不做重复判断    


flush_else() 把数据上传到 ES 以后删除内存里的数据  



### 08.06 项目总体介绍  

#### 基本工具  

connect 包：各种连接方式、ES 还多一个分页方法，这个方法比较重要，用的比较多。  

kafka 有一个生产和消费，send 生产，receive 是消费。get_consumer 指定消费者  

MySQL 就一个连接  


filters 包：全是匹配部分  

base.py 传一组规则，传入规则名，同义词、限定词、反向限定词，主要是查国家用的。高级搜索里使用。key_rules。用于匹配。可以同时匹配多条规则。  

add_key 一组规则，add_keys 多组规则。exec_filter 是查询用的，传入规则  

reverse_keyfilter_result()，把结果转换了一种返回形式。    

esm 传很多关键词，匹配一句话就能把命中的关键词和位置筛出来，在 index 的 AutoIndex 类中。  


tools 包：各种工具  

mx_simhash.py 不看  

text_tools.py 扩展的文本处理，有各种各样的处理方法，可以链式调用（返回了 self）（有很大的改进空间）  

threadingadapter.py 多线程，多进程，返回结果队列传个 piplines 中使用    

utils.py 延迟加载，不重要  


piplines 包：接受数据、处理数据、返回数据，异步运行（多线程，多进程）  

linemodule.py last_step_queue 接收 threadingadapter.py 中处理完成的数据，主要有 3 个方法，read_data 读数据，normalize_data 预处理，process_data 是正规处理数据。  

db_read_module.py 从数据库批量读数据，返回数据  

es_bulk_module.py 批量操作 ES，主要是增删改    

es_craw_module.py 主要是从 ES 中查数据   

forecast_module.py 筛选包好预测关键词的文章，去除不在的  

lac_module.py 提取人名、机构、关键词，专家观点。给产品进行规则分析用的。  

origin_post_module.py 没用，不看  

output_module.py 转换成 Excel 格式。不用看  

zhili_module.py  选取有治理关键词的文章  



#### user 应用

下 utils.py trans_to_md5 把所有传入的内容转换成 md5，有一个生成验证码的函数，不用管。  

views.py   


#### post 应用  

前端展示的，几乎所有方法都有缓存功能，可以指定过期时间，因为数据要每天更新，所以可以指定时间过期  

ZKYCache 类，操控 Redis，缓存是用 Redis 实现的  

get_by_args 函数，根据传的参数取 name  

HotPostsView 就是 5 篇文章，每天更新，先尝试获取缓存，  

important_search 是重点资讯  














